# -*- coding: utf-8 -*-
"""Untitled63.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ncc70a_6WtqR8sSmLI-Sdm4HM6M-bnrs
"""

!pip install tensorflow_datasets -q

import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras import layers
from tensorflow.keras.models import Model
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input

(train_ds_raw, val_ds_raw), ds_info = tfds.load(
    'eurosat/rgb',
    split=['train[:80%]', 'train[80%:]'],
    shuffle_files=True,
    as_supervised=True,
    with_info=True,
)

# Extracting class
CLASS_NAMES = ds_info.features['label'].names
NUM_CLASSES = len(CLASS_NAMES)

print(f"Number of training samples: {len(train_ds_raw)}")
print(f"Number of validation samples: {len(val_ds_raw)}")
print(f"Number of classes: {NUM_CLASSES}")
print(f"Class Names: {CLASS_NAMES}")


IMG_SIZE = 224
BATCH_SIZE = 32

def preprocess_image(image, label):
    """
    Resizes image to IMG_SIZE x IMG_SIZE and applies MobileNetV2-specific preprocessing.
    """
    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
    image = preprocess_input(image)
    return image, label

train_ds = train_ds_raw.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
train_ds = train_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)

val_ds = val_ds_raw.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
val_ds = val_ds.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)

IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)

# Load base
base_model = MobileNetV2(
    input_shape=IMG_SHAPE,
    include_top=False,
    weights='imagenet'
)

# Freeze base model
base_model.trainable = False


inputs = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(inputs, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.2)(x)
outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)

model = Model(inputs, outputs)

# Compile
model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

from tensorflow.keras.callbacks import ModelCheckpoint
initial_epochs = 10

checkpoint_path = "eurosat_model_checkpoint.weights.h5"
model_checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_path,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

print("--- Starting initial training... ---")

history = model.fit(
    train_ds,
    epochs=initial_epochs,
    validation_data=val_ds,
    callbacks=[model_checkpoint_callback]  # <-- Add the callback here
)

# After training, loading the best weights that were saved by the checkpoint
print(f"\nTraining complete. Loading best model weights from {checkpoint_path}...")
model.load_weights(checkpoint_path)
print("Best weights loaded into model.")

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')

plt.subplot(1, 2, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')

plt.tight_layout()
plt.show()